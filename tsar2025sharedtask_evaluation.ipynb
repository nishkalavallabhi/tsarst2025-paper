{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GJlv1BjiyE7"
   },
   "source": [
    "# Evaluation Script for TSAR 2025 Shared Task on Readability-Controlled Text Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAVOaDYdi4Jd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install --upgrade huggingface_hub\n",
    "!pip install bert_score\n",
    "!pip install evaluate\n",
    "!pip install -U dspy pydantic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2pRsP1d088y"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,root_mean_squared_error\n",
    "from transformers import pipeline,AutoTokenizer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXt8mUY50-i5"
   },
   "outputs": [],
   "source": [
    "\"\"\"from huggingface_hub import login\n",
    "login()\n",
    "\"\"\"\n",
    "SEED = 42                           # for reproducibility\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- IO --------------------\n",
    "def read_jsonl(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def read_gold(path: str):\n",
    "    data = read_jsonl(path)\n",
    "    if not data:\n",
    "        raise ValueError(f\"Gold file is empty: {path}\")\n",
    "    try:\n",
    "        original = [e[\"original\"] for e in data]\n",
    "        reference = [e[\"reference\"] for e in data]\n",
    "        target   = [e[\"target_cefr\"] for e in data]   # case handled later\n",
    "        text_ids = [e[\"text_id\"] for e in data]\n",
    "    except KeyError as ke:\n",
    "        raise KeyError(f\"Gold file missing key {ke}. First item keys: {list(data[0].keys())}\")\n",
    "    return original, reference, target, text_ids\n",
    "\n",
    "def read_submission(path: str):\n",
    "    data = read_jsonl(path)\n",
    "    if not data:\n",
    "        raise ValueError(f\"Submission is empty: {path}\")\n",
    "    first_keys = list(data[0].keys())\n",
    "    if \"simplified\" not in data[0]:\n",
    "        raise KeyError(f\"{path} must contain 'simplified'. Found keys: {first_keys}\")\n",
    "    if \"text_id\" not in data[0]:\n",
    "        raise KeyError(f\"{path} must contain 'text_id'. Found keys: {first_keys}\")\n",
    "    return [e[\"simplified\"] for e in data], [e[\"text_id\"] for e in data], len(data)\n",
    "\n",
    "# Align system outputs to ANY overlapping gold ids (supports partial submissions)\n",
    "def align_intersection(hyps, sys_ids, gold_ids, gold_orig, gold_ref, gold_tgt):\n",
    "    gid2idx = {g:i for i,g in enumerate(gold_ids)}\n",
    "    pairs = [(gid2idx[sid], hyp) for hyp, sid in zip(hyps, sys_ids) if sid in gid2idx]\n",
    "    if not pairs:\n",
    "        return None\n",
    "    pairs.sort(key=lambda x: x[0])\n",
    "    sel_idx = [i for i,_ in pairs]\n",
    "    aligned_hyps = [h for _,h in pairs]\n",
    "    aligned_orig = [gold_orig[i] for i in sel_idx]\n",
    "    aligned_ref  = [gold_ref[i]  for i in sel_idx]\n",
    "    aligned_tgt  = [gold_tgt[i]  for i in sel_idx]\n",
    "    coverage_n   = len(sel_idx)\n",
    "    coverage_pct = round(100.0 * coverage_n / len(gold_ids), 2)\n",
    "    missing_ids  = [g for g in gold_ids if g not in set(sys_ids)]\n",
    "    extra_ids    = [s for s in sys_ids if s not in set(gold_ids)]\n",
    "    return {\n",
    "        \"hyps\": aligned_hyps,\n",
    "        \"orig\": aligned_orig,\n",
    "        \"ref\":  aligned_ref,\n",
    "        \"tgt\":  aligned_tgt,\n",
    "        \"coverage_n\": coverage_n,\n",
    "        \"coverage_pct\": coverage_pct,\n",
    "        \"missing_ids\": missing_ids,\n",
    "        \"extra_ids\": extra_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TgsXrd5jBx9"
   },
   "outputs": [],
   "source": [
    "# ------------- Models/Metrics -----------\n",
    "BATCH_SIZE = 32                     # adjust for your GPU\n",
    "\n",
    "cefr_labeler1 = pipeline(\"text-classification\",\n",
    "    model=\"AbdullahBarayan/ModernBERT-base-doc_en-Cefr\", device=0, torch_dtype=\"auto\")\n",
    "cefr_labeler2 = pipeline(\"text-classification\",\n",
    "    model=\"AbdullahBarayan/ModernBERT-base-doc_sent_en-Cefr\", device=0, torch_dtype=\"auto\")\n",
    "cefr_labeler3 = pipeline(\"text-classification\",\n",
    "    model=\"AbdullahBarayan/ModernBERT-base-reference_AllLang2-Cefr2\", device=0, torch_dtype=\"auto\")\n",
    "\n",
    "meaning_bert = evaluate.load(\"davebulaval/meaningbert\")\n",
    "bertscore    = evaluate.load(\"bertscore\")\n",
    "\n",
    "CEFR = [\"A1\",\"A2\",\"B1\",\"B2\",\"C1\",\"C2\"]\n",
    "L2I  = {l:i for i,l in enumerate(CEFR)}\n",
    "\n",
    "def cefr_labels(hyps, models, batch_size=BATCH_SIZE):\n",
    "    p1 = models[0](hyps, batch_size=batch_size, truncation=True)\n",
    "    p2 = models[1](hyps, batch_size=batch_size, truncation=True)\n",
    "    p3 = models[2](hyps, batch_size=batch_size, truncation=True)\n",
    "    def top1(x):\n",
    "        if isinstance(x, dict): return x\n",
    "        if isinstance(x, list) and x: return max(x, key=lambda d: d[\"score\"])\n",
    "    outs = []\n",
    "    for d1, d2, d3 in zip(p1, p2, p3):\n",
    "        best = max((top1(d1), top1(d2), top1(d3)), key=lambda d: d[\"score\"])\n",
    "        outs.append(best[\"label\"].strip().upper())\n",
    "    return outs\n",
    "\n",
    "def score_cefr(hyps, ref_lvls, models):\n",
    "    gold  = [str(l).strip().upper() for l in ref_lvls]\n",
    "    preds = [str(l).strip().upper() for l in cefr_labels(hyps, models, batch_size=BATCH_SIZE)]\n",
    "    f1 = f1_score(gold, preds, average=\"weighted\")\n",
    "    t  = np.array([L2I[l] for l in gold])\n",
    "    p  = np.array([L2I[l] for l in preds])\n",
    "    adj  = (np.abs(t - p) <= 1).mean()\n",
    "    rmse = root_mean_squared_error(t, p)\n",
    "    return {\"weighted_f1\": round(float(f1),4),\n",
    "            \"adj_accuracy\": round(float(adj),4),\n",
    "            \"rmse\": round(float(rmse),4)}\n",
    "\n",
    "def score_meaningbert(hyps, refs):\n",
    "    res = meaning_bert.compute(predictions=hyps, references=refs)\n",
    "    return round(float(np.mean(res[\"scores\"])) / 100.0, 4)\n",
    "\n",
    "def score_bertscore(hyps, refs, scoretype=\"f1\"):\n",
    "    res = bertscore.compute(references=refs, predictions=hyps, lang=\"en\")\n",
    "    return round(float(np.mean(res[scoretype])), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Main ---------------------\n",
    "\n",
    "GOLD_FILE = \"tsar2025_test_withrefs.jsonl\"\n",
    "\n",
    "if not os.path.isfile(GOLD_FILE):\n",
    "    raise FileNotFoundError(f\"Gold file not found: {GOLD_FILE}\")\n",
    "gold_orig, gold_ref, gold_tgt, gold_ids = read_gold(GOLD_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE RUN PATH HERE:\n",
    "run_path = \"output_dspyreacttool_paper_geminiflash.jsonl\" \n",
    "hyps, sys_ids, num_instances = read_submission(run_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = align_intersection(hyps, sys_ids, gold_ids, gold_orig, gold_ref, gold_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval():\n",
    "    if aligned is None:\n",
    "                print(f\"no overlap with gold; skipping.\")\n",
    "                row = {\"modelname\": run_path, \n",
    "                       \"num_instances\": num_instances,\n",
    "                       \"coverage_n\": 0, \"coverage_pct\": 0.0,\n",
    "                       \"weighted_f1\": \"n/a\", \"adj_accuracy\": \"n/a\", \"rmse\": \"n/a\",\n",
    "                       \"meaningbert-orig\": \"n/a\", \"bertscore-orig\": \"n/a\",\n",
    "                       \"meaningbert-ref\": \"n/a\", \"bertscore-ref\": \"n/a\"}\n",
    "    else:\n",
    "                if aligned[\"missing_ids\"]:\n",
    "                    print(f\"missing {len(aligned['missing_ids'])} ids.\")\n",
    "                if aligned[\"extra_ids\"]:\n",
    "                    print(f\"extra {len(aligned['extra_ids'])} ids (ignored).\")\n",
    "                hyps_i, orig_i, ref_i, tgt_i = aligned[\"hyps\"], aligned[\"orig\"], aligned[\"ref\"], aligned[\"tgt\"]\n",
    "                cefr = score_cefr(hyps_i, tgt_i, [cefr_labeler1, cefr_labeler2, cefr_labeler3])\n",
    "                mb_o = score_meaningbert(hyps_i, orig_i)\n",
    "                bs_o = score_bertscore(hyps_i, orig_i, \"f1\")\n",
    "                mb_r = score_meaningbert(hyps_i, ref_i)\n",
    "                bs_r = score_bertscore(hyps_i, ref_i, \"f1\")\n",
    "                row = {\"modelname\": run_path, \n",
    "                       \"num_instances\": num_instances,\n",
    "                       \"coverage_n\": aligned[\"coverage_n\"],\n",
    "                       \"coverage_pct\": aligned[\"coverage_pct\"],\n",
    "                       \"weighted_f1\": cefr[\"weighted_f1\"], \"adj_accuracy\": cefr[\"adj_accuracy\"], \"rmse\": cefr[\"rmse\"],\n",
    "                       \"meaningbert-orig\": mb_o, \"bertscore-orig\": bs_o,\n",
    "                       \"meaningbert-ref\": mb_r, \"bertscore-ref\": bs_r}\n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(do_eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3.13",
   "language": "python",
   "name": "python3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
